{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning Project nÂ°2:\n",
    "# <center>Text Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Tweets offer a gigantic database, that hints at useful informations about Natural Language Processing. They have the advantage of being constrained in size, which offers us a limitation in the number of words per tweet and will refer to one emotion from the user. This emotion can take various form, but for our analysis we will stick to a positive or negative sentiment. This jupyter notebook shows the different approaches we followed to determine the feeling of various tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/neuro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/neuro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/neuro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# If error -> pip install --upgrade gensim\n",
    "\n",
    "from tqdm import tqdm, tnrange\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"Process\")\n",
    "# If error -> pip install tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "# If error -> pip install -U nltk\n",
    "nltk.download('wordnet') #Uncomment this line if first time using 'wordnet' corpus\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "lmtzr = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data pre-processing\n",
    "\n",
    "### 1.1 Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import positive and negative tweets \n",
    "tweet_pos_df = pd.read_csv('twitter-datasets/train_pos_full.txt', \n",
    "                           names=['text'], delimiter=\"\\t\", header=None)\n",
    "tweet_pos_df['sentiment'] = 1\n",
    "\n",
    "tweet_neg_df = pd.read_csv('twitter-datasets/train_neg_full.txt', \n",
    "                           names=['text'], delimiter=\"\\t\", header=None)\n",
    "tweet_neg_df['sentiment'] = -1\n",
    "\n",
    "# Create the general dataframe\n",
    "tweets_df = tweet_pos_df.append(tweet_neg_df)\n",
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "del tweet_pos_df, tweet_neg_df\n",
    "\n",
    "tweets_df = tweets_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guess i wasnt any help</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eben say hi to me please you would make my who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steve madden's are really nice but why so maha...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dingo bone , large , 3-1 / 2 - ounce ( grocery...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; haha , not true , but thanks ... pretty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; thanks jason</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; follow &lt;user&gt; hes the guy you were list...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;user&gt; lilo and stitch ! wow brings back memories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ugh ! ! why does my dog feel the need to const...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my mums so much more tanned then me #paleshit ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                             guess i wasnt any help         -1\n",
       "1  eben say hi to me please you would make my who...          1\n",
       "2  steve madden's are really nice but why so maha...         -1\n",
       "3  dingo bone , large , 3-1 / 2 - ounce ( grocery...         -1\n",
       "4  <user> haha , not true , but thanks ... pretty...          1\n",
       "5                                <user> thanks jason          1\n",
       "6  <user> follow <user> hes the guy you were list...          1\n",
       "7  <user> lilo and stitch ! wow brings back memories          1\n",
       "8  ugh ! ! why does my dog feel the need to const...         -1\n",
       "9  my mums so much more tanned then me #paleshit ...         -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tweets processing\n",
    "- Remove all punctation & all numbers (maybe not useful)\n",
    "- Remove all user & url mention\n",
    "- Remove stop words\n",
    "- Tokenize (split) each tweet\n",
    "- Tag word (noun, verb, adj, adv...)\n",
    "- Lematize words\n",
    "- Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_dict = {'N' : 'n', 'V' : 'v', 'J' : 'a', 'S' : 's', 'R' : 'r'}\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def convert_tag_to_pos(tag): #(for lemmatizer)\n",
    "    \"\"\" Convert the tag given by pos_tag() into something lemmatize can understand \"\"\"\n",
    "    if tag in pos_dict.keys():\n",
    "        return pos_dict[tag]\n",
    "    else:\n",
    "        return 'n' #Default value\n",
    "\n",
    "def get_tokenized_tweet(tweet): \n",
    "    \"\"\" Clean tweet (remove punctations and numbers and tokenize it)\"\"\"\n",
    "    # Remove all punctation & all numbers\n",
    "    tweet = re.sub('[^A-Za-z ]+','', tweet)\n",
    "    \n",
    "    # Remove user & url\n",
    "    #tweet = re.sub('user', '', tweet)                            \n",
    "    #tweet = re.sub('url', '', tweet)\n",
    "    \n",
    "    tokens = TweetTokenizer().tokenize(tweet)\n",
    "    filtered_sentence = []\n",
    " \n",
    "    # Stop words filtering\n",
    "    #for w in tokens:\n",
    "    #    if w not in stop_words:\n",
    "    #        filtered_sentence.append(w)\n",
    "    #tokens = filtered_sentence\n",
    "    #del filtered_sentence\n",
    "   \n",
    "    # Lemmatization \n",
    "    #tokens = [lmtzr.lemmatize(word,convert_tag_to_pos(tag)) for word,tag in tagged]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [sb.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb64ff37d714c5bb7e6cfcee4e8bb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guess i wasnt any help</td>\n",
       "      <td>-1</td>\n",
       "      <td>[guess, i, wasnt, ani, help]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eben say hi to me please you would make my who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[eben, say, hi, to, me, pleas, you, would, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steve madden's are really nice but why so maha...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[steve, madden, are, realli, nice, but, whi, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dingo bone , large , 3-1 / 2 - ounce ( grocery...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[dingo, bone, larg, ounc, groceri, dingo, bone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; haha , not true , but thanks ... pretty...</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, haha, not, true, but, thank, pretti, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; thanks jason</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, thank, jason]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; follow &lt;user&gt; hes the guy you were list...</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, follow, user, hes, the, guy, you, were,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;user&gt; lilo and stitch ! wow brings back memories</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, lilo, and, stitch, wow, bring, back, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ugh ! ! why does my dog feel the need to const...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[ugh, whi, doe, my, dog, feel, the, need, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my mums so much more tanned then me #paleshit ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[my, mum, so, much, more, tan, then, me, pales...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0                             guess i wasnt any help         -1   \n",
       "1  eben say hi to me please you would make my who...          1   \n",
       "2  steve madden's are really nice but why so maha...         -1   \n",
       "3  dingo bone , large , 3-1 / 2 - ounce ( grocery...         -1   \n",
       "4  <user> haha , not true , but thanks ... pretty...          1   \n",
       "5                                <user> thanks jason          1   \n",
       "6  <user> follow <user> hes the guy you were list...          1   \n",
       "7  <user> lilo and stitch ! wow brings back memories          1   \n",
       "8  ugh ! ! why does my dog feel the need to const...         -1   \n",
       "9  my mums so much more tanned then me #paleshit ...         -1   \n",
       "\n",
       "                                      tokenized text  \n",
       "0                       [guess, i, wasnt, ani, help]  \n",
       "1  [eben, say, hi, to, me, pleas, you, would, mak...  \n",
       "2  [steve, madden, are, realli, nice, but, whi, s...  \n",
       "3  [dingo, bone, larg, ounc, groceri, dingo, bone...  \n",
       "4  [user, haha, not, true, but, thank, pretti, su...  \n",
       "5                               [user, thank, jason]  \n",
       "6  [user, follow, user, hes, the, guy, you, were,...  \n",
       "7  [user, lilo, and, stitch, wow, bring, back, me...  \n",
       "8  [ugh, whi, doe, my, dog, feel, the, need, to, ...  \n",
       "9  [my, mum, so, much, more, tan, then, me, pales...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tokenized text'] = tweets_df['text'].progress_map(get_tokenized_tweet)\n",
    "\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the word2vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data in test and train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df['tokenized text'], \n",
    "                                                    tweets_df['sentiment'],\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "del tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process tweets for the word2vec model\n",
    "- Create word2vec model\n",
    "\n",
    "We wante to compute the word2vec model to transform any tweet in a vector. Once we have a vector for each word, we can get the associated sentence's vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_taggedDocument(X): \n",
    "    \"\"\" Prepare taggedDocument class from tweets for model\"\"\"\n",
    "    taggedDocument = []\n",
    "    \n",
    "    for index in tqdm_notebook(range(len(X))):\n",
    "        words = X[index]\n",
    "        tags = \"TRAIN_\" + str(index)\n",
    "        \n",
    "        taggedDocument.append(TaggedDocument(words, tags))\n",
    "        \n",
    "    return taggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9312bada97204ba5964a7a56309dde97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Build vocab...\n",
      "Train w2v model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94798253"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedDocument = get_taggedDocument(list(X_train))\n",
    "all_words = [x.words for x in taggedDocument]\n",
    "n_dim = 250\n",
    "\n",
    "# Creation of the model\n",
    "w2v_model = Word2Vec(size=n_dim, min_count=2,workers = 4) # Test with different min_count !\n",
    "\n",
    "# Build vocab\n",
    "print(\"Build vocab...\")\n",
    "w2v_model.build_vocab(all_words)\n",
    "\n",
    "# Train the model\n",
    "print(\"Train w2v model...\")\n",
    "w2v_model.train(all_words,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=w2v_model.iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goood', 0.7205197215080261),\n",
       " ('great', 0.7098403573036194),\n",
       " ('bad', 0.6105893850326538),\n",
       " ('nice', 0.5894044637680054),\n",
       " ('terribl', 0.5668543577194214),\n",
       " ('fab', 0.5608239769935608),\n",
       " ('horribl', 0.555421769618988),\n",
       " ('rubbish', 0.5260864496231079),\n",
       " ('decent', 0.5183205008506775),\n",
       " ('gud', 0.5112592577934265)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 tf-idf matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfâidf or TFIDF: numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tf-idf matrix\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform(all_words)\n",
    "tf_idf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tweet_to_vector(tweet, size):\n",
    "    \"\"\" Convert a tweet in a vector based on the w2v model\"\"\"\n",
    "    vector = np.zeros(size).reshape((1, size))\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector += w2v_model[word].reshape((1, size)) * tf_idf[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "    return vector\n",
    "\n",
    "def get_vectors(tweets):\n",
    "    \"\"\" Create the matrix associated to a set of tweets\"\"\"\n",
    "    n_dim=250\n",
    "    all_vectors = [convert_tweet_to_vector(tweet, n_dim) for tweet in tqdm_notebook(tweets)]\n",
    "    \n",
    "    return np.concatenate(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fa5228527c43168b5e988ae6234033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d25630e9d4a3db7db35aea6e4a197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the vectors matrix from tweets\n",
    "X_train_w2v = get_vectors(X_train)\n",
    "del X_train\n",
    "X_test_w2v  = get_vectors(X_test)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-07, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (50, 50, 50)\n",
    "classifierPercZ = MLPClassifier(solver='adam', alpha=1e-7, hidden_layer_sizes = size, random_state=1)\n",
    "classifierPercZ.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score on the test set is: 0.829487043892\n"
     ]
    }
   ],
   "source": [
    "print(\"The score on the test set is:\", classifierPercZ.score(X_test_w2v, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Test on the unlabelized twitter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113dc8c167c64515a78e4b1d23f7cd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file created !\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "tweet_unlabelized_df = pd.read_csv('twitter-datasets/test_data.txt', \n",
    "                                   names=['text'], delimiter=\"\\t\", header=None)\n",
    "tweet_unlabelized_index = tweet_unlabelized_df.index.values + 1\n",
    "\n",
    "# Process data\n",
    "X_unlabelized = tweet_unlabelized_df['text'].map(get_tokenized_tweet)\n",
    "X_unlabelized_w2v = get_vectors(X_unlabelized)\n",
    "\n",
    "# Make predictions\n",
    "y_unlabelized = classifierPercZ.predict(X_unlabelized_w2v)\n",
    "\n",
    "# Create submission file\n",
    "create_csv_submission(tweet_unlabelized_index, y_unlabelized, \"text_class_submission_top.csv\")\n",
    "print(\"Submission file created !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
